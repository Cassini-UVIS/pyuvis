{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp idlsav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDLSAV\n",
    "> Provides `read_idlsav_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.io import readsav\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_kernels(cell):\n",
    "    if all(np.unique(cell) == np.array([''])):\n",
    "        return np.NAN\n",
    "    else:\n",
    "        return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def swap_cell(cell):\n",
    "    return cell.byteswap().newbyteorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_idlsav_file(filename):\n",
    "    tmp = readsav(filename)\n",
    "    # find largest substructure and return that\n",
    "    key_of_longest = ''\n",
    "    longestlength = 0\n",
    "    print(\"Searching for longest substructure and returning that.\")\n",
    "    for thiskey in tmp.keys():\n",
    "        try:\n",
    "            thislength = len(tmp[thiskey])\n",
    "        except TypeError:\n",
    "            print(\"Item with key '{}' has no length. Skipping.\"\n",
    "                  .format(thiskey))\n",
    "            continue\n",
    "        print(\"Found '{}'' with length {}.\".format(thiskey, thislength))\n",
    "        if thislength > longestlength:\n",
    "            key_of_longest = thiskey\n",
    "            longestlength = thislength\n",
    "\n",
    "    print(\"Return substructure with name '{}'.\".format(key_of_longest))\n",
    "    df = pd.DataFrame.from_records(tmp[key_of_longest])\n",
    "\n",
    "    #print df.dtypes.values\n",
    "\n",
    "    # There is a problem here. You see all these '>f8' datatypes in there?\n",
    "    # This means that at least those parts of the IDL data structure is in the\n",
    "    # inverse byte-ordering way than today's modern PC use, horrible!!\n",
    "    # One needs to convert this, otherwise all the numbers can't be trusted.\n",
    "\n",
    "    # print df.head()\n",
    "    #print df.UVIS[0]\n",
    "\n",
    "    # This looks like each cell has a 2D numpy array:\n",
    "    # print df.UVIS[0].shape\n",
    "    # print df.columns.values\n",
    "\n",
    "    # Ok, let's change the columns that can be changed easily, i.e. the columns\n",
    "    # that don't have arrays in each cell:\n",
    "    df = df.apply(lambda x: x.values.byteswap().newbyteorder()\n",
    "                  if x.dtype != 'O' else x)\n",
    "    ## print df.dtypes.values\n",
    "\n",
    "    # As you can see, the float dtypes are now pointing to the left ('<f8')\n",
    "    # which means they are little-endian, as any normal computer is these days.\n",
    "    # Now, let's put a proper index, `time` for instance:\n",
    "\n",
    "    #df.index = pd.DatetimeIndex(df.UTC)\n",
    "    ## print df.drop('UTC', axis=1, inplace=True)\n",
    "    ## print df.index\n",
    "\n",
    "    # Now, swap the bytes for each array in each DataFrame cell\n",
    "    # first loop over columns, and then eeach column get's the lambda converter\n",
    "    # func from above.\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != np.dtype('O'):\n",
    "            try:\n",
    "                df[col] = df[col].map(swap_cell)\n",
    "            except TypeError:\n",
    "                print(col, 'typeerror')\n",
    "    #print df.UVIS.iloc[0].dtype\n",
    "    #print df.dtypes.values\n",
    "\n",
    "    # TODO: put this stuff later in o clean-up function\n",
    "    # success!!\n",
    "    # Now let's see if the KERNELS column actually ever has data:\n",
    "    # df.KERNELS = df.KERNELS.map(check_kernels)\n",
    "    ## print 'wtf', df.KERNELS.notnull().value_counts()\n",
    "\n",
    "    # This means that no KERNEL data was included, so we can drop it:\n",
    "    # print df.drop('KERNELS', axis=1, inplace=True)\n",
    "    # first only look at easy columns where there isn't an array per cell:\n",
    "    # dtypecheck = df.dtypes != 'O'\n",
    "    # easy_cols = dtypecheck[dtypecheck is True].index\n",
    "\n",
    "    # note you can scroll the table to the right, but if it's wider or longer\n",
    "    # than a certain number (settable) than it's truncated\n",
    "    #print df[easy_cols]\n",
    "    # Now it's your turn! ;)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SavConverter:\n",
    "    def __init__(self, filename):\n",
    "        self.fname = filename\n",
    "        tmp = readsav(filename)\n",
    "        # find largest substructure and return that\n",
    "        key_of_longest = ''\n",
    "        longestlength = 0\n",
    "        print(\"Searching for longest substructure and returning that.\")\n",
    "        for thiskey in tmp.keys():\n",
    "            try:\n",
    "                thislength = len(tmp[thiskey])\n",
    "            except TypeError:\n",
    "                print(\"Item with key '{}' has no length. Skipping.\"\n",
    "                      .format(thiskey))\n",
    "                continue\n",
    "            print(\"Found '{}'' with length {}.\".format(thiskey, thislength))\n",
    "            if thislength > longestlength:\n",
    "                key_of_longest = thiskey\n",
    "                longestlength = thislength\n",
    "\n",
    "        print(\"Return substructure with name '{}'.\".format(key_of_longest))\n",
    "        self.df = pd.DataFrame.from_records(tmp[key_of_longest])\n",
    "\n",
    "    @property\n",
    "    def dtypes(self):\n",
    "        return self.df.dtypes.values\n",
    "    #print df.dtypes.values\n",
    "\n",
    "    # There is a problem here. You see all these '>f8' datatypes in there?\n",
    "    # This means that at least those parts of the IDL data structure is in the\n",
    "    # inverse byte-ordering way than today's modern PC use, horrible!!\n",
    "    # One needs to convert this, otherwise all the numbers can't be trusted.\n",
    "\n",
    "    # print df.head()\n",
    "    #print df.UVIS[0]\n",
    "\n",
    "    # This looks like each cell has a 2D numpy array:\n",
    "    # print df.UVIS[0].shape\n",
    "    # print df.columns.values\n",
    "\n",
    "    # Ok, let's change the columns that can be changed easily, i.e. the columns\n",
    "    # that don't have arrays in each cell:\n",
    "#     df = df.apply(lambda x: x.values.byteswap().newbyteorder()\n",
    "#                   if x.dtype != 'O' else x)\n",
    "    ## print df.dtypes.values\n",
    "\n",
    "    # As you can see, the float dtypes are now pointing to the left ('<f8')\n",
    "    # which means they are little-endian, as any normal computer is these days.\n",
    "    # Now, let's put a proper index, `time` for instance:\n",
    "\n",
    "    #df.index = pd.DatetimeIndex(df.UTC)\n",
    "    ## print df.drop('UTC', axis=1, inplace=True)\n",
    "    ## print df.index\n",
    "\n",
    "    # Now, swap the bytes for each array in each DataFrame cell\n",
    "    # first loop over columns, and then eeach column get's the lambda converter\n",
    "    # func from above.\n",
    "\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype != np.dtype('O'):\n",
    "#             try:\n",
    "#                 df[col] = df[col].map(swap_cell)\n",
    "#             except TypeError:\n",
    "#                 print(col, 'typeerror')\n",
    "    #print df.UVIS.iloc[0].dtype\n",
    "    #print df.dtypes.values\n",
    "\n",
    "    # TODO: put this stuff later in o clean-up function\n",
    "    # success!!\n",
    "    # Now let's see if the KERNELS column actually ever has data:\n",
    "    # df.KERNELS = df.KERNELS.map(check_kernels)\n",
    "    ## print 'wtf', df.KERNELS.notnull().value_counts()\n",
    "\n",
    "    # This means that no KERNEL data was included, so we can drop it:\n",
    "    # print df.drop('KERNELS', axis=1, inplace=True)\n",
    "    # first only look at easy columns where there isn't an array per cell:\n",
    "    # dtypecheck = df.dtypes != 'O'\n",
    "    # easy_cols = dtypecheck[dtypecheck is True].index\n",
    "\n",
    "    # note you can scroll the table to the right, but if it's wider or longer\n",
    "    # than a certain number (settable) than it's truncated\n",
    "    #print df[easy_cols]\n",
    "    # Now it's your turn! ;)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/home/maye/Dropbox/SternchenAndMe/python_stuff/FUV2005_195_19_52_08_UVIS_011EN_ICYEXO001_PRIME.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for longest substructure and returning that.\n",
      "Found 'datastruct2'' with length 71.\n",
      "Item with key 'datastruct2_initial' has no length. Skipping.\n",
      "Item with key 'datastruct2_final' has no length. Skipping.\n",
      "Return substructure with name 'datastruct2'.\n"
     ]
    }
   ],
   "source": [
    "conv = SavConverter(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('>f8'), dtype('>f8'),\n",
       "       dtype('>f8'), dtype('>f8'), dtype('>f8'), dtype('>f8'),\n",
       "       dtype('>f8'), dtype('>f8'), dtype('>f8'), dtype('>f8'),\n",
       "       dtype('>f8'), dtype('>f8'), dtype('>f8'), dtype('>f8'),\n",
       "       dtype('>f8'), dtype('>f8'), dtype('>f8'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('>i4'), dtype('>i4'), dtype('>i4'), dtype('>i4'),\n",
       "       dtype('>i4'), dtype('>i4'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('O'), dtype('O')], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
